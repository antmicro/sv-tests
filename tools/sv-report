#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Copyright (C) 2020 The SymbiFlow Authors.
#
# Use of this source code is governed by a ISC-style
# license that can be found in the LICENSE file or at
# https://opensource.org/licenses/ISC
#
# SPDX-License-Identifier: ISC

from pygments.formatters import HtmlFormatter
from pygments import lexers, highlight
import multiprocessing
from glob import glob
from logparser import parseLog
import argparse
import logging
import jinja2
import markupsafe
import csv
import datetime
import sys
import os
import re
import html
import urllib.parse
import dataclasses
import enum
from typing import Dict, Any, List, Set, Iterable
import json

parser = argparse.ArgumentParser()

logger_args = parser.add_mutually_exclusive_group()

logger_args.add_argument(
    "-q", "--quiet", action="store_true", help="Disable all logs")

logger_args.add_argument(
    "-v", "--verbose", action="store_true", help="Verbose logging")

parser.add_argument(
    "-i", "--input", help="Input database/LRM", default="conf/lrm.conf")

parser.add_argument(
    "-m",
    "--meta-tags",
    help="Meta-tags config file",
    default="conf/meta-tags.conf")

parser.add_argument(
    "-l",
    "--logs",
    help="Directory with all the individual test results",
    default="out/logs")

parser.add_argument(
    "--template",
    help="Path to the html report template",
    default="conf/report/report-template.html")

parser.add_argument(
    "--code-template",
    help="Path to the html code preview template",
    default="conf/report/code-template.html")

parser.add_argument(
    "--log-template",
    help="Path to the html log template",
    default="conf/report/log-template.html")

parser.add_argument(
    "-o",
    "--out",
    help="Path to the html file with the report",
    default="out/report/index.html")

parser.add_argument(
    "-c",
    "--csv",
    help="Path to the csv file with the report",
    default="out/report/report.csv")

parser.add_argument(
    "-r", "--revision", help="Report revision", default="unknown")

# We only consider tests with minimum this size for the throughput
# calculation, so that we skip the super-tiny few-line tests that are
# not a true reflection of a common usage.
minimum_throughput_file_size = 1024

# parse args
args = parser.parse_args()

# setup logger
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)

ch = logging.StreamHandler()
ch.setFormatter(logging.Formatter('%(levelname)-8s| %(message)s'))
logger.addHandler(ch)

if args.quiet:
    logger.setLevel(logging.ERROR)
elif args.verbose:
    logger.setLevel(logging.DEBUG)
else:
    logger.setLevel(logging.INFO)

# Common paths
out_dir = os.path.dirname(os.path.abspath(args.out))
top_dir = os.path.abspath(os.curdir)
logs_out_dir = os.path.join(out_dir, "logs")

lex = lexers.get_lexer_by_name("systemverilog")


def escapeXmlAttribute(value: str):
    return str(markupsafe.escape(value)).replace("\n", "&#10;")


def escapeJsString(value: str):
    def esc(match: re.Match):
        return rf"\x{ord(match.group(0)):02x}"

    return re.sub(r"[\x00-\x1F\x7F'\"]", esc, value)


def surroundWithQuotes(value: str, quot='"'):
    return quot + str(value) + quot


# NOTE: this works correctly only with numbers shorter than 10 digits
def numericSortKey(s: str):
    # prepend all number occurences with the length of the number
    r = re.sub(r"\d+", lambda m: str(len(m.group(0))) + m.group(0), s).lower()
    return r


def numericSort(l: List[str]):
    return sorted(l, key=numericSortKey)


def numericDictSort(d: Dict[str, Any]):
    return sorted(d.items(), key=lambda kv: numericSortKey(kv[0]))


# Initialize Jinja2 environment with custom filter

jinja2env = jinja2.Environment(trim_blocks=True, lstrip_blocks=True)
jinja2env.filters["escape_attr"] = escapeXmlAttribute
jinja2env.filters["escape_js_str"] = escapeJsString
jinja2env.filters["quote"] = surroundWithQuotes
jinja2env.filters["numeric_sort"] = numericSort
jinja2env.filters["numeric_dictsort"] = numericDictSort


def exists_and_is_newer_than(target: str, sources: List[str]):
    if not os.path.exists(target):
        return False

    for s in sources:
        if (not os.path.exists(s)
                or os.path.getctime(s) > os.path.getctime(target)):
            return False

    return True


def totalSize(files):
    size = 0
    for f in files:
        try:
            size += os.path.getsize(f)
        except FileNotFoundError:
            pass
    return size


def criticalError(msg):
    logger.critical(msg)
    sys.exit(1)


def readConfig(filename):
    config = {}
    urls = {}
    try:
        with open(filename) as f:
            for l in f:
                ls = l.strip()
                # skip lines with comments
                if re.search(r"^\s*#.*", ls) is not None:
                    continue

                entry = ls.split("\t")

                if not 2 <= len(entry) <= 3:
                    raise KeyError("Invalid entry: " + ls)

                config[entry[0]] = entry[1]

                # check if we
                if len(entry) == 3:
                    urls[entry[0]] = entry[2]
    except (OSError, FileNotFoundError, KeyError) as e:
        criticalError(f"Unable to parse {config} file - {str(e)}")
    return config, urls


# generate input database first
database, urls = readConfig(args.input)

# read meta-tags configuration
meta_tags, meta_urls = readConfig(args.meta_tags)

urls = {**urls, **meta_urls}


@enum.unique
class TestStatus(enum.Enum):
    NA = "test-na"
    PASSED = "test-passed"
    FAILED = "test-failed"
    VARIED = "test-varied"

    def __str__(self):
        return self.value


@dataclasses.dataclass
class TestResult:
    log_html_file: str = ""

    name: str = ""
    tags: Set[str] = dataclasses.field(default_factory=set)
    types: Set[str] = dataclasses.field(default_factory=set)
    results_group: str = ""
    input_files: List[str] = dataclasses.field(default_factory=list)
    # Unit: bytes
    total_input_files_size: int = 0
    status: TestStatus = TestStatus.NA
    exit_code: int = 0

    # Unit: seconds
    timeout: int = 0

    # Unit: seconds
    total_time: float = 0
    user_time: float = 0
    system_time: float = 0

    # Unit: KB
    ram_usage: float = 0


@dataclasses.dataclass
class TagInfo:
    description: str = ""
    url: str = ""


@dataclasses.dataclass
class ToolInfo:
    # Tool name should be used only for display purposes. Always use a key
    # from `tools` in `ReportResults` (or another dict where tool ID is
    # the key) as a runner/tool ID.
    name: str = ""
    version: str = ""
    url: str = ""


@dataclasses.dataclass
class TagToolResult:
    tests: List[TestResult] = dataclasses.field(default_factory=list)
    types: Set[str] = dataclasses.field(default_factory=set)

    passed_tests: int = 0

    @property
    def total_tests(self):
        return len(self.tests)

    @property
    def status(self) -> TestStatus:
        if (self.total_tests == 0):
            return TestStatus.NA
        elif (self.passed_tests == 0):
            return TestStatus.FAILED
        elif (self.passed_tests == self.total_tests):
            return TestStatus.PASSED
        return TestStatus.VARIED


@dataclasses.dataclass
class ToolSummary:
    # Unit: seconds
    total_time: float = 0
    user_time: float = 0
    system_time: float = 0

    # Unit: MB
    max_ram_usage: float = 0

    # Unit: KiB/s
    passed_throughput: float = 0

    total_passed_tests: int = 0
    total_tests: int = 0
    total_passed_tags: int = 0
    total_tested_tags: int = 0


@dataclasses.dataclass
class GroupData:
    # key 1 = tag id; key 2 = runner name = runner class name
    tags_tools: Dict[str, Dict[str, TagToolResult]] = dataclasses.field(
        default_factory=dict)
    # key = runner name = runner class name
    summaries: Dict[str, ToolSummary] = dataclasses.field(default_factory=dict)
    # key = runner name = runner class name
    tests: Dict[str, List[TestResult]] = dataclasses.field(default_factory=dict)


# GroupData equivalent for data from a single tool
@dataclasses.dataclass
class PartialGroupData:
    # key 1 = tag id
    tags: Dict[str, TagToolResult] = dataclasses.field(default_factory=dict)
    summary: ToolSummary = dataclasses.field(default_factory=ToolSummary)
    tests: List[TestResult] = dataclasses.field(default_factory=list)


@dataclasses.dataclass
class ReportResults:
    # Results groups (key = results group id)
    groups: Dict[str, GroupData] = dataclasses.field(default_factory=dict)
    # key = runner name = runner class name
    tools: Dict[str, ToolInfo] = dataclasses.field(default_factory=dict)

    # Tags used in the report. Keys are tag IDs.
    tags: Dict[str, TagInfo] = dataclasses.field(default_factory=dict)


# Data passed to report template
report_data = ReportResults()

PATH_WITH_LINE_NO_MATCHER = re.compile(
    r"([^\s:\"'`/]*/[^\s:\"'`]+\.\w+)(:(\d+)(?::\d+)?)?")


def convertPathsToRelativeLinks(text: str, relative_to: str):
    def convertPath(match: re.Match):
        path: str = html.unescape(match.group(1))
        if not os.path.exists(path):
            return match.group(0)

        if path.startswith(top_dir):
            path = path[len(top_dir) + 1:]
        elif path.startswith("/"):
            return match.group(0)

        url = os.path.join(out_dir, path + ".html")
        url = os.path.relpath(url, relative_to)
        url = urllib.parse.quote(url)
        esc_match = str(markupsafe.escape(path))
        line_no = match.group(3)
        if line_no is not None:
            url = f"{url}#l-{line_no}"
            esc_match = esc_match + (match.group(2) or "")
        return f'<a href="{url}" target="file-frame">{esc_match}</a>'

    return PATH_WITH_LINE_NO_MATCHER.sub(convertPath, text)


with open(args.log_template, "r") as templ:
    log_template = jinja2env.from_string(templ.read())


def renderLogToHTMLFile(
        out_dir: str, log_html_path: str, test_result: TestResult,
        test_log: Dict[str, str], log_content: str):
    files_map: Dict[str, str] = {}
    log_html_dir = os.path.dirname(log_html_path)
    for input_file in test_result.input_files:
        # Absolute path:
        input_file_html = os.path.join(out_dir, input_file + ".html")
        # Path relative to rendered log:
        input_file_html = os.path.relpath(input_file_html, log_html_dir)

        files_map[input_file] = urllib.parse.quote(input_file_html)

    log_content = convertPathsToRelativeLinks(
        str(markupsafe.escape(log_content)), log_html_dir)

    csspath = os.path.join(out_dir, "log.css")
    csspath = os.path.relpath(csspath, os.path.dirname(log_html_path))
    os.makedirs(os.path.dirname(log_html_path), exist_ok=True)
    with open(log_html_path, 'w') as f:
        f.write(
            log_template.render(
                input_files_map=files_map,
                result=test_result,
                log=test_log,
                content=log_content,
                csspath=csspath))


with open(args.code_template, "r") as templ:
    src_template = jinja2env.from_string(templ.read())


def renderInputFileToHTMLFile(
        out_dir: str, html_path: str, input_file_path: str):
    formatter = HtmlFormatter(
        full=False, linenos=True, anchorlinenos=True, lineanchors='l')

    os.makedirs(os.path.dirname(html_path), exist_ok=True)

    raw_code = ""
    try:
        with open(input_file_path, 'rb') as f:
            raw_code = f.read()
    except IOError:
        logger.warning(f"Error when opening file {input_file_path}")
        try:
            with open(html_path, 'w') as out:
                out.write('Error when opening file ' + input_file_path)
        except:
            pass
        return

    code = highlight(raw_code, lex, formatter)
    csspath = os.path.join(out_dir, "code.css")
    csspath = os.path.relpath(csspath, os.path.dirname(html_path))
    with open(html_path, 'w') as out:
        out.write(
            src_template.render(
                csspath=csspath, filename=input_file_path, code=code))


def renderTagResultsConfig(
        js_path: str, tool: str, tag: str, test_results: Iterable[TestResult]):
    tool = tool.lower()
    tag = tag.lower()
    js_global_variable_name = f"config_loader_data['{tool}/{tag}']"

    # Order of values in each entry:
    # [name, status, log_html_path, first_input_file_html_path]
    config = []
    for test_result in test_results:
        name = test_result.name
        status = 1 if test_result.status == TestStatus.PASSED else 0
        log_html_file = test_result.log_html_file
        first_input_html = test_result.input_files[0] + ".html"
        first_input_html = urllib.parse.quote(first_input_html)
        config.append([name, status, log_html_file, first_input_html])

    js_data = json.dumps(config, separators=(",", ":"))
    code = f"{js_global_variable_name} = {js_data}"

    os.makedirs(os.path.dirname(js_path), exist_ok=True)
    with open(js_path, "w") as f:
        f.write(code)


def collect_logs(runner_name: str):
    @dataclasses.dataclass
    class LocalPartialGroupData:
        group: PartialGroupData = dataclasses.field(
            default_factory=PartialGroupData)

        # Values used to calculate throughput. Totals are collected only for
        # files with size > minimum_throughput_file_size
        passed_tests_time: float = 0.0
        passed_tests_input_files_size: float = 0.0

    local_groups: Dict[str, LocalPartialGroupData] = {}
    tool_info: ToolInfo = ToolInfo()

    rendered_count = 0
    tests_count = 0

    for log_file in glob(os.path.join(args.logs, runner_name, "**/*.log"),
                         recursive=True):
        # Strip f"{args.logs}/" prefix from log file path
        t_id = log_file[len(args.logs) + 1:]
        logger.debug("Found log: " + t_id)

        # Tests that have not run will have an existing, but empty logfile.
        if os.path.getsize(log_file) == 0:
            continue

        test_result = TestResult()

        remaining_parameters = {
            "name", "tags", "should_fail", "rc", "date_completed",
            "description", "files", "incdirs", "top_module", "runner",
            "runner_url", "time_elapsed", "type", "mode", "timeout",
            "user_time", "system_time", "ram_usage", "tool_success",
            "should_fail_because", "defines", "compatible-runners",
            "results_group"
        }
        test_log_data: Dict[str, Any] = {}
        log_content = ""

        with open(log_file, "r") as f:
            try:
                for l in f:
                    attr = re.search(r"^([a-zA-Z_-]+):(.+)", l)

                    if attr is None:
                        raise KeyError(
                            "Could not find parameters: {}".format(
                                ", ".join(remaining_parameters)))

                    param = attr.group(1).lower()
                    value = attr.group(2).strip()

                    if param not in remaining_parameters:
                        logger.warning(
                            "Skipping unknown parameter: {} in {}".format(
                                param, log_file))
                        continue
                    if param in test_log_data:
                        logger.warning(
                            "Skipping duplicated parameter: {} in {}".format(
                                param, log_file))
                        continue

                    test_log_data[param] = value

                    remaining_parameters.remove(param)
                    if len(remaining_parameters) == 0:
                        # found all tags
                        break

            except Exception as e:
                logger.warning(
                    "Skipping {} on {}: {}".format(
                        log_file, runner_name, str(e)))
                continue

            log_content = f.read()

        # Test Result

        test_result.name = test_log_data["name"]
        test_result.results_group = test_log_data["results_group"]

        # Convert splitted "tags" to set() and append all meta-tags
        test_result.tags = set(test_log_data["tags"].split())
        for meta_tag, dependency_tags in meta_tags.items():
            dependency_tags = set(dependency_tags.split())
            if not dependency_tags.isdisjoint(test_result.tags):
                test_result.tags.add(meta_tag)

        test_result.input_files = [
            os.path.relpath(f, top_dir)
            for f in test_log_data["files"].split()
        ]

        test_result.types = test_log_data["type"].split()
        test_result.total_input_files_size = totalSize(test_result.input_files)
        test_result.exit_code = int(test_log_data["rc"])

        # Determine test status
        tool_should_fail = test_log_data["should_fail"] == "1"
        tool_crashed = test_result.exit_code >= 126
        tool_failed = test_log_data["tool_success"] == "0"
        if tool_crashed or tool_should_fail != tool_failed:
            test_result.status = TestStatus.FAILED
        elif (test_log_data["mode"] == "simulation"
              and not parseLog(log_content)):
            test_result.status = TestStatus.FAILED
        else:
            test_result.status = TestStatus.PASSED

        test_result.timeout = int(test_log_data["timeout"])

        test_result.total_time = float(test_log_data["time_elapsed"])
        test_result.user_time = float(test_log_data["user_time"])
        test_result.system_time = float(test_log_data["system_time"])

        test_result.ram_usage = float(test_log_data["ram_usage"])  # KB

        log_html = os.path.join(logs_out_dir, t_id + ".html")
        test_result.log_html_file = os.path.relpath(log_html, out_dir)

        # Render the log if needed
        if not exists_and_is_newer_than(
                log_html, [log_file, args.log_template, __file__]):
            rendered_count += 1
            renderLogToHTMLFile(
                out_dir, log_html, test_result, test_log_data, log_content)
        tests_count += 1

        # Group

        local_group = local_groups.setdefault(
            test_result.results_group, LocalPartialGroupData())
        group = local_group.group
        group.tests.append(test_result)

        # (tag, tool) results

        for tag in test_result.tags:
            tag_data = group.tags.setdefault(tag, TagToolResult())
            tag_data.tests.append(test_result)
            tag_data.types.update(test_result.types)
            if test_result.status == TestStatus.PASSED:
                tag_data.passed_tests += 1

        # Summary

        if test_result.status == TestStatus.PASSED:
            input_files_size = test_result.total_input_files_size
            if input_files_size > minimum_throughput_file_size:
                local_group.passed_tests_input_files_size += input_files_size
                local_group.passed_tests_time += test_result.total_time

        summary = group.summary

        summary.total_time += test_result.total_time
        summary.user_time += test_result.user_time
        summary.system_time += test_result.system_time

        ram_usage_mb = test_result.ram_usage / 1000
        if summary.max_ram_usage < ram_usage_mb:
            summary.max_ram_usage = ram_usage_mb

        summary.total_tests += 1
        if test_result.status == TestStatus.PASSED:
            summary.total_passed_tests += 1

        if tool_info.name == "":
            tool_info.name = test_log_data["runner"]

    logger.info(
        f"{runner_name}: (Re)generated {rendered_count}/{tests_count} rendered log files."
    )

    for group_id, local_group in local_groups.items():
        group = local_group.group
        for tag_id, tag_tool_result in group.tags.items():
            if tag_tool_result.status != TestStatus.NA:
                group.summary.total_tested_tags += 1
                if tag_tool_result.status == TestStatus.PASSED:
                    group.summary.total_passed_tags += 1

            tag_tool_result.tests.sort(key=lambda t: numericSortKey(t.name))
            config_js = os.path.join(
                out_dir, "results", runner_name.lower(),
                f"{tag_id.lower()}.config.js")

            # FIXME: render tests from ALL group_id
            # Render the config
            renderTagResultsConfig(config_js, runner_name, tag_id, tag_tool_result.tests)

        time = local_group.passed_tests_time
        size = local_group.passed_tests_input_files_size
        if time == 0:
            group.summary.passed_throughput = 0
        else:
            group.summary.passed_throughput = size / time / 1024

    try:
        with open(os.path.join(args.logs, runner_name, "version")) as f:
            tool_info.version = f.read().strip()
    except:
        pass

    try:
        with open(os.path.join(args.logs, runner_name, "url")) as f:
            tool_info.url = f.read().strip()
    except:
        pass

    groups: Dict[str, PartialGroupData] = {
        id: lg.group
        for id, lg in local_groups.items()
    }

    return {
        "tool_info": tool_info,
        "partial_groups": groups,
    }

# CamelCase or undscore_separator csv headers ?
# We use CamelCase so that gnuplot getting header from CSV displays them
# as-is and doesn't print the post-underscore letter a subscript.
csv_header = [
    'TestName',
    'Tool',  # Parser/Compiler/Tool processing it
    'Group',
    'Pass',  # result. True if we got expected result.
    'ExitCode',  # Actual tool exit code
    'Tags',
    'InputBytes',  # test facts
    'AllowedTimeout',  # Some measurements
    'TimeUser',
    'TimeSystem',
    'TimeWall',
    'RamUsageMiB'
]
csv_output = {}

# Input files (.sv) path relative to repository's toplevel directory
all_input_files: Set[str] = set()

runner_names = []
for r in [os.path.dirname(r) for r in glob(args.logs + "/*/")]:
    runner_name = os.path.basename(r)
    logger.debug("Found Runner: " + runner_name)
    runner_names.append(runner_name)

logger.info("Generating {} from log files in '{}'".format(args.out, args.logs))

with multiprocessing.Pool() as pool:
    results = pool.map(collect_logs, runner_names)

for tool_name, tool_results in zip(runner_names, results):
    report_data.tools[tool_name] = tool_results["tool_info"]
    partial_groups: Dict[str, PartialGroupData] = tool_results["partial_groups"]

    for group_id, partial_group in partial_groups.items():
        group = report_data.groups.setdefault(group_id, GroupData())
        group.tests[tool_name] = partial_group.tests
        group.summaries[tool_name] = partial_group.summary

        for tag_id, tool_data in partial_group.tags.items():
            tools = group.tags_tools.setdefault(tag_id, {})
            tools[tool_name] = tool_data

            if tag_id not in database and tag_id not in report_data.tags:
                logger.warning("Tag not present in the database: " + tag_id)
            report_data.tags.setdefault(
                tag_id,
                TagInfo(
                    description=database.get(tag_id, ""),
                    url=urls.get(tag_id, "")))

        for test_result in partial_group.tests:
            all_input_files.update(test_result.input_files)

            unique_row = test_result.name + ":" + tool_name

            csv_output[unique_row] = {
                "TestName": test_result.name,
                "Tool": tool_name,
                "Group": test_result.results_group,
                "Pass": test_result.status == TestStatus.PASSED,
                "ExitCode": test_result.exit_code,
                "Tags": " ".join(test_result.tags),
                "InputBytes": test_result.total_input_files_size,
                "AllowedTimeout": test_result.timeout,
                "TimeUser": round(test_result.user_time, 6),
                "TimeSystem": round(test_result.system_time, 6),
                "TimeWall": round(test_result.total_time, 6),
                "RamUsageMiB": round(test_result.ram_usage / 1024, 3),
            }

# Render input files
rendered_count = 0
for input_file in all_input_files:
    input_file_html = os.path.join(out_dir, input_file + ".html")
    if exists_and_is_newer_than(input_file_html,
                                [input_file, args.code_template, __file__]):
        continue
    rendered_count += 1
    renderInputFileToHTMLFile(out_dir, input_file_html, input_file)

logger.info(
    f"(Re)generated {rendered_count}/{len(all_input_files)} rendered input files."
)

try:
    with open(args.template, "r") as f:
        report = jinja2env.from_string(f.read())

    build_id = os.environ.get('GITHUB_RUN_ID', 'local')
    build_datetime = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    with open(args.out, 'w') as f:
        f.write(
            report.render(
                report=report_data,
                revision=args.revision,
                build_id=build_id,
                datetime=build_datetime))

    with open(args.csv, 'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=csv_header)
        writer.writeheader()
        for test in csv_output:
            writer.writerow(csv_output[test])
except KeyError:
    logger.critical("Unable to generate report, not enough logs")
except Exception as e:
    logger.critical("Unable to generate report: " + str(e))
