#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Copyright (C) 2020 The SymbiFlow Authors.
#
# Use of this source code is governed by a ISC-style
# license that can be found in the LICENSE file or at
# https://opensource.org/licenses/ISC
#
# SPDX-License-Identifier: ISC

from pygments.formatters import HtmlFormatter
from pygments import lexers, highlight
import multiprocessing
from glob import glob
from logparser import parseLog
import argparse
import logging
import jinja2
import markupsafe
import csv
import datetime
import sys
import os
import re
import html
import urllib.parse
import dataclasses
import enum
from typing import Dict, Any, List, Set, Iterable
import json

parser = argparse.ArgumentParser()

logger_args = parser.add_mutually_exclusive_group()

logger_args.add_argument(
    "-q", "--quiet", action="store_true", help="Disable all logs")

logger_args.add_argument(
    "-v", "--verbose", action="store_true", help="Verbose logging")

parser.add_argument(
    "-i", "--input", help="Input database/LRM", default="conf/lrm.conf")

parser.add_argument(
    "-m",
    "--meta-tags",
    help="Meta-tags config file",
    default="conf/meta-tags.conf")

parser.add_argument(
    "-l",
    "--logs",
    help="Directory with all the individual test results",
    default="out/logs")

parser.add_argument(
    "--template",
    help="Path to the html report template",
    default="conf/report/report-template.html")

parser.add_argument(
    "--code-template",
    help="Path to the html code preview template",
    default="conf/report/code-template.html")

parser.add_argument(
    "--log-template",
    help="Path to the html log template",
    default="conf/report/log-template.html")

parser.add_argument(
    "-o",
    "--out",
    help="Path to the html file with the report",
    default="out/report/index.html")

parser.add_argument(
    "-c",
    "--csv",
    help="Path to the csv file with the report",
    default="out/report/report.csv")

parser.add_argument(
    "-r", "--revision", help="Report revision", default="unknown")

# We only consider tests with minimum this size for the throughput
# calculation, so that we skip the super-tiny few-line tests that are
# not a true reflection of a common usage.
minimum_throughput_file_size = 1024

# parse args
args = parser.parse_args()

# setup logger
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)

ch = logging.StreamHandler()
ch.setFormatter(logging.Formatter('%(levelname)-8s| %(message)s'))
logger.addHandler(ch)

if args.quiet:
    logger.setLevel(logging.ERROR)
elif args.verbose:
    logger.setLevel(logging.DEBUG)
else:
    logger.setLevel(logging.INFO)

out_dir = os.path.dirname(os.path.abspath(args.out))
top_dir = os.path.abspath(os.curdir)

lex = lexers.get_lexer_by_name("systemverilog")


def escapeXmlAttribute(value: str):
    return str(markupsafe.escape(value)).replace("\n", "&#10;")


def escapeJsString(value: str):
    def esc(match: re.Match):
        return rf"\x{ord(match.group(0)):02x}"

    return re.sub(r"[\x00-\x1F\x7F'\"]", esc, value)


def surroundWithQuotes(value: str, quot='"'):
    return quot + str(value) + quot


# Initialize Jinja2 environment with custom filter

jinja2env = jinja2.Environment(trim_blocks=True, lstrip_blocks=True)
jinja2env.filters["escape_attr"] = escapeXmlAttribute
jinja2env.filters["escape_js_str"] = escapeJsString
jinja2env.filters["quote"] = surroundWithQuotes


def exists_and_is_newer_than(target: str, sources: List[str]):
    if not os.path.exists(target):
        return False

    for s in sources:
        if (not os.path.exists(s)
                or os.path.getctime(s) > os.path.getctime(target)):
            return False

    return True


def totalSize(files):
    size = 0
    for f in files:
        try:
            size += os.path.getsize(f)
        except FileNotFoundError:
            pass
    return size


def criticalError(msg):
    logger.critical(msg)
    sys.exit(1)


# class used for sorting "test tabs" in the report
class TestResultComp(object):
    def __init__(self, item):
        self.item = item

    def prepend_nums(self, s):
        # prepend all number occurences with the length of the number
        for m in re.findall(r'\d+', s):
            s = s.replace(m, str(len(m)) + m)

        return s

    def __lt__(self, other):
        s = self.prepend_nums(self.item.name)
        o = self.prepend_nums(other.item.name)

        return s < o


def readConfig(filename):
    config = {}
    urls = {}
    try:
        with open(filename) as f:
            for l in f:
                ls = l.strip()
                # skip lines with comments
                if re.search(r"^\s*#.*", ls) is not None:
                    continue

                entry = ls.split("\t")

                if not 2 <= len(entry) <= 3:
                    raise KeyError("Invalid entry: " + ls)

                config[entry[0]] = entry[1]

                # check if we
                if len(entry) == 3:
                    urls[entry[0]] = entry[2]
    except (OSError, FileNotFoundError, KeyError) as e:
        criticalError(f"Unable to parse {config} file - {str(e)}")
    return config, urls


# generate input database first
database, urls = readConfig(args.input)

# read meta-tags configuration
meta_tags, meta_urls = readConfig(args.meta_tags)

urls = {**urls, **meta_urls}


@enum.unique
class TestStatus(enum.Enum):
    NA = "test-na"
    PASSED = "test-passed"
    FAILED = "test-failed"
    VARIED = "test-varied"

    def __str__(self):
        return self.value


@dataclasses.dataclass
class TestResult:
    log_html_file: str = ""

    name: str = ""
    tags: Set[str] = dataclasses.field(default_factory=set)
    types: Set[str] = dataclasses.field(default_factory=set)
    input_files: List[str] = dataclasses.field(default_factory=list)
    # Unit: bytes
    total_input_files_size: int = 0
    status: TestStatus = TestStatus.NA
    exit_code: int = 0

    # Unit: seconds
    timeout: int = 0

    # Unit: seconds
    total_time: float = 0
    user_time: float = 0
    system_time: float = 0

    # Unit: KB
    ram_usage: float = 0


@dataclasses.dataclass
class TagResults:
    status: TestStatus = TestStatus.NA
    types: List[str] = dataclasses.field(default_factory=list)
    passed_tests: int = 0
    tests: List[TestResult] = dataclasses.field(default_factory=list)


@dataclasses.dataclass
class ToolResults:
    tests: Dict[str, TestResult] = dataclasses.field(default_factory=dict)
    tags: Dict[str, TagResults] = dataclasses.field(default_factory=dict)

    # Unit: seconds
    total_time: float = 0
    user_time: float = 0
    system_time: float = 0

    # Unit: MB
    max_ram_usage: float = 0

    # Unit: KiB/s
    passed_throughput: float = 0

    # Tool name. This should be used only for display purposes. Always use
    # a key from `tools` in `ReportResults` as a runner/tool ID.
    name: str = ""
    version: str = ""
    url: str = ""

    total_passed_tests: int = 0
    total_passed_tags: int = 0
    total_tested_tags: int = 0

    @property
    def total_tests(self):
        return len(self.tests)


@dataclasses.dataclass
class TagInfo:
    description: str = ""
    url: str = ""


@dataclasses.dataclass
class ReportResults:
    # Results from each runner/tool.
    # Keys are runner names (i.e. runner class names)
    tools: Dict[str, ToolResults] = dataclasses.field(default_factory=dict)
    # Tags used in the report. Keys are tag IDs.
    tags: Dict[str, TagInfo] = dataclasses.field(default_factory=dict)


# Data passed to report template
report_data = ReportResults()

PATH_WITH_LINE_NO_MATCHER = re.compile(
    r"([^\s:\"'`/]*/[^\s:\"'`]+\.\w+)(:(\d+)(?::\d+)?)?")


def convertPathsToRelativeLinks(text: str, relative_to: str):
    def convertPath(match: re.Match):
        path: str = html.unescape(match.group(1))
        if not os.path.exists(path):
            return match.group(0)

        if path.startswith(top_dir):
            path = path[len(top_dir) + 1:]
        elif path.startswith("/"):
            return match.group(0)

        url = os.path.join(out_dir, path + ".html")
        url = os.path.relpath(url, relative_to)
        url = urllib.parse.quote(url)
        esc_match = str(markupsafe.escape(path))
        line_no = match.group(3)
        if line_no is not None:
            url = f"{url}#l-{line_no}"
            esc_match = esc_match + (match.group(2) or "")
        return f'<a href="{url}" target="file-frame">{esc_match}</a>'

    return PATH_WITH_LINE_NO_MATCHER.sub(convertPath, text)


with open(args.log_template, "r") as templ:
    log_template = jinja2env.from_string(templ.read())


def renderLogToHTMLFile(
        out_dir: str, log_html_path: str, test_result: TestResult,
        test_log: Dict[str, str], log_content: str):
    files_map: Dict[str, str] = {}
    log_html_dir = os.path.dirname(log_html_path)
    for input_file in test_result.input_files:
        # Absolute path:
        input_file_html = os.path.join(out_dir, input_file + ".html")
        # Path relative to rendered log:
        input_file_html = os.path.relpath(input_file_html, log_html_dir)

        files_map[input_file] = urllib.parse.quote(input_file_html)

    log_content = convertPathsToRelativeLinks(
        str(markupsafe.escape(log_content)), log_html_dir)

    os.makedirs(os.path.dirname(log_html_path), exist_ok=True)
    with open(log_html_path, 'w') as f:
        f.write(
            log_template.render(
                input_files_map=files_map,
                result=test_result,
                log=test_log,
                content=log_content))


with open(args.code_template, "r") as templ:
    src_template = jinja2env.from_string(templ.read())


def renderInputFileToHTMLFile(
        out_dir: str, html_path: str, input_file_path: str):
    formatter = HtmlFormatter(
        full=False, linenos=True, anchorlinenos=True, lineanchors='l')

    os.makedirs(os.path.dirname(html_path), exist_ok=True)

    raw_code = ""
    try:
        with open(input_file_path, 'rb') as f:
            raw_code = f.read()
    except IOError:
        logger.warning(f"Error when opening file {input_file_path}")
        try:
            with open(html_path, 'w') as out:
                out.write('Error when opening file ' + input_file_path)
        except:
            pass
        return

    code = highlight(raw_code, lex, formatter)
    csspath = os.path.join(out_dir, "code.css")
    csspath = os.path.relpath(csspath, os.path.dirname(html_path))
    with open(html_path, 'w') as out:
        out.write(
            src_template.render(
                csspath=csspath, filename=input_file_path, code=code))


def renderTagResultsConfig(
        js_path: str, tool: str, tag: str, test_results: Iterable[TestResult]):
    tool = tool.lower()
    tag = tag.lower()
    js_global_variable_name = f"config_loader_data['{tool}/{tag}']"

    # Order of values in each entry:
    # [name, status, log_html_path, first_input_file_html_path]
    config = []
    for test_result in test_results:
        name = test_result.name
        status = 1 if test_result.status == TestStatus.PASSED else 0
        log_html_file = test_result.log_html_file
        first_input_html = test_result.input_files[0] + ".html"
        first_input_html = urllib.parse.quote(first_input_html)
        config.append([name, status, log_html_file, first_input_html])

    js_data = json.dumps(config, separators=(",", ":"))
    code = f"{js_global_variable_name} = {js_data}"

    os.makedirs(os.path.dirname(js_path), exist_ok=True)
    with open(js_path, "w") as f:
        f.write(code)


def collect_logs(runner_name: str):
    tool_results = ToolResults()

    # Values used to calculate throughput. Totals are collected only for files
    # with size > minimum_throughput_file_size
    passed_tests_time = 0.0
    passed_tests_input_files_size = 0.0

    rendered_count = 0

    @dataclasses.dataclass
    class TagStatuses:
        passed_count: int = 0
        present_statuses: Set[TestStatus] = dataclasses.field(
            default_factory=set)
        present_types: Set[str] = dataclasses.field(default_factory=set)

    tags_statuses: Dict[str, TagStatuses] = {}

    for log_file in glob(os.path.join(args.logs, runner_name, "**/*.log"),
                         recursive=True):
        # Strip f"{args.logs}/" prefix from log file path
        t_id = log_file[len(args.logs) + 1:]
        logger.debug("Found log: " + t_id)

        # Tests that have not run will have an existing, but empty logfile.
        if os.path.getsize(log_file) == 0:
            continue

        test_result = TestResult()

        remaining_parameters = {
            "name", "tags", "should_fail", "rc", "date_completed",
            "description", "files", "incdirs", "top_module", "runner",
            "runner_url", "time_elapsed", "type", "mode", "timeout",
            "user_time", "system_time", "ram_usage", "tool_success",
            "should_fail_because", "defines", "compatible-runners"
        }
        test_log_data: Dict[str, Any] = {}
        log_content = ""

        with open(log_file, "r") as f:
            try:
                for l in f:
                    attr = re.search(r"^([a-zA-Z_-]+):(.+)", l)

                    if attr is None:
                        raise KeyError(
                            "Could not find parameters: {}".format(
                                ", ".join(remaining_parameters)))

                    param = attr.group(1).lower()
                    value = attr.group(2).strip()

                    if param not in remaining_parameters:
                        logger.warning(
                            "Skipping unknown parameter: {} in {}".format(
                                param, log_file))
                        continue
                    if param in test_log_data:
                        logger.warning(
                            "Skipping duplicated parameter: {} in {}".format(
                                param, log_file))
                        continue

                    test_log_data[param] = value

                    remaining_parameters.remove(param)
                    if len(remaining_parameters) == 0:
                        # found all tags
                        break

            except Exception as e:
                logger.warning(
                    "Skipping {} on {}: {}".format(
                        log_file, runner_name, str(e)))
                continue

            log_content = f.read()

        tool_results.tests[t_id] = test_result

        if tool_results.name == "":
            tool_results.name = test_log_data["runner"]

        test_result.name = test_log_data["name"]

        # Convert splitted "tags" to set() and append all meta-tags
        test_result.tags = set(test_log_data["tags"].split())
        for meta_tag, dependency_tags in meta_tags.items():
            dependency_tags = set(dependency_tags.split())
            if not dependency_tags.isdisjoint(test_result.tags):
                test_result.tags.add(meta_tag)

        test_result.input_files = [
            os.path.relpath(f, top_dir)
            for f in test_log_data["files"].split()
        ]

        test_result.types = test_log_data["type"].split()
        test_result.total_input_files_size = totalSize(test_result.input_files)
        test_result.exit_code = int(test_log_data["rc"])

        # Determine test status
        tool_should_fail = test_log_data["should_fail"] == "1"
        tool_crashed = test_result.exit_code >= 126
        tool_failed = test_log_data["tool_success"] == "0"

        if tool_crashed or tool_should_fail != tool_failed:
            test_result.status = TestStatus.FAILED
        elif test_log_data["mode"] == "simulation" and not parseLog(
                log_content):
            test_result.status = TestStatus.FAILED
        else:
            test_result.status = TestStatus.PASSED
            tool_results.total_passed_tests += 1
            # Collect stats for calculating passed_throughput
            input_files_size = test_result.total_input_files_size
            if input_files_size > minimum_throughput_file_size:
                passed_tests_input_files_size += input_files_size
                passed_tests_time += float(test_log_data["time_elapsed"])

        test_result.timeout = int(test_log_data["timeout"])

        test_result.total_time = float(test_log_data["time_elapsed"])
        test_result.user_time = float(test_log_data["user_time"])
        test_result.system_time = float(test_log_data["system_time"])

        tool_results.total_time += test_result.total_time
        tool_results.user_time += test_result.user_time
        tool_results.system_time += test_result.system_time

        test_result.ram_usage = float(test_log_data["ram_usage"])  # KB
        ram_usage_mb = test_result.ram_usage / 1000
        if tool_results.max_ram_usage < ram_usage_mb:
            tool_results.max_ram_usage = ram_usage_mb

        logs_out_dir = os.path.join(out_dir, "logs")
        log_html = os.path.join(logs_out_dir, t_id + ".html")

        test_result.log_html_file = os.path.relpath(log_html, out_dir)

        # Render the log if needed
        if not exists_and_is_newer_than(
                log_html, [log_file, args.log_template, __file__]):
            rendered_count += 1
            renderLogToHTMLFile(
                out_dir, log_html, test_result, test_log_data, log_content)

        for tag in test_result.tags:
            tag_statuses = tags_statuses.setdefault(tag, TagStatuses())
            if test_result.status == TestStatus.PASSED:
                tag_statuses.passed_count += 1
            tag_statuses.present_statuses.add(test_result.status)
            tag_statuses.present_types.update(test_result.types)

    for tag, tag_statuses in tags_statuses.items():
        tag_results = tool_results.tags[tag] = TagResults()

        tag_results.passed_tests = tag_statuses.passed_count
        if len(tag_statuses.present_statuses) == 0:
            tag_results.status = TestStatus.NA
        elif len(tag_statuses.present_statuses) == 1:
            tag_results.status = list(tag_statuses.present_statuses)[0]
            tool_results.total_tested_tags += 1
            if (tag_results.status == TestStatus.PASSED):
                tool_results.total_passed_tags += 1
        else:
            tag_results.status = TestStatus.VARIED
            tool_results.total_tested_tags += 1

        tag_results.types = list(tag_statuses.present_types)

        tag_results.tests = sorted(
            [t for t in tool_results.tests.values() if tag in t.tags],
            key=TestResultComp)

        config_js = os.path.join(
            out_dir, "results", runner_name.lower(),
            f"{tag.lower()}.config.js")

        # Render the config
        renderTagResultsConfig(config_js, runner_name, tag, tag_results.tests)

    try:
        with open(os.path.join(args.logs, runner_name, "version")) as f:
            tool_results.version = f.read().strip()
    except:
        pass

    try:
        with open(os.path.join(args.logs, runner_name, "url")) as f:
            tool_results.url = f.read().strip()
    except:
        pass

    if passed_tests_time == 0:
        tool_results.passed_throughput = 0
    else:
        tool_results.passed_throughput = (
            passed_tests_input_files_size / passed_tests_time / 1024)

    logger.info(
        f"{runner_name}: (Re)generated {rendered_count}/{tool_results.total_tests} rendered log files."
    )

    return tool_results


logger.info("Generating {} from log files in '{}'".format(args.out, args.logs))

# Input files (.sv) path relative to repository's toplevel directory
all_input_files: Set[str] = set()

runner_names = []

for r in [os.path.dirname(r) for r in glob(args.logs + "/*/")]:
    runner_name = os.path.basename(r)
    logger.debug("Found Runner: " + runner_name)

    runner_names.append(runner_name)

with multiprocessing.Pool() as pool:
    results = pool.map(collect_logs, runner_names)

for tool_name, tool_results in zip(runner_names, results):
    report_data.tools[tool_name] = tool_results
    for tag in tool_results.tags:
        if tag not in database and tag not in report_data.tags:
            logger.warning("Tag not present in the database: " + tag)
        report_data.tags.setdefault(
            tag,
            TagInfo(description=database.get(tag, ""), url=urls.get(tag, "")))
    for test_result in tool_results.tests.values():
        all_input_files.update(test_result.input_files)

# Render input files
rendered_count = 0
for input_file in all_input_files:
    input_file_html = os.path.join(out_dir, input_file + ".html")
    if exists_and_is_newer_than(input_file_html,
                                [input_file, args.code_template, __file__]):
        continue
    rendered_count += 1
    renderInputFileToHTMLFile(out_dir, input_file_html, input_file)

logger.info(
    f"(Re)generated {rendered_count}/{len(all_input_files)} rendered input files."
)

# CamelCase or undscore_separator csv headers ?
# We use CamelCase so that gnuplot getting header from CSV displays them
# as-is and doesn't print the post-underscore letter a subscript.
csv_header = [
    'TestName',
    'Tool',  # Parser/Compiler/Tool processing it
    'Pass',  # result. True if we got expected result.
    'ExitCode',  # Actual tool exit code
    'Tags',
    'InputBytes',  # test facts
    'AllowedTimeout',  # Some measurements
    'TimeUser',
    'TimeSystem',
    'TimeWall',
    'RamUsageMiB'
]

csv_output = {}

for tool_name, tool_results in report_data.tools.items():
    for test_result in tool_results.tests.values():
        unique_row = test_result.name + ":" + tool_name

        csv_output[unique_row] = {
            "TestName": test_result.name,
            "Tool": tool_name,
            "Pass": test_result.status == TestStatus.PASSED,
            "ExitCode": test_result.exit_code,
            "Tags": " ".join(test_result.tags),
            "InputBytes": test_result.total_input_files_size,
            "AllowedTimeout": test_result.timeout,
            "TimeUser": round(test_result.user_time, 6),
            "TimeSystem": round(test_result.system_time, 6),
            "TimeWall": round(test_result.total_time, 6),
            "RamUsageMiB": round(test_result.ram_usage / 1024, 3),
        }

try:
    with open(args.template, "r") as f:
        report = jinja2env.from_string(f.read())

    build_id = os.environ.get('GITHUB_RUN_ID', 'local')
    build_datetime = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    with open(args.out, 'w') as f:
        f.write(
            report.render(
                report=report_data,
                revision=args.revision,
                build_id=build_id,
                datetime=build_datetime))

    with open(args.csv, 'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=csv_header)
        writer.writeheader()
        for test in csv_output:
            writer.writerow(csv_output[test])
except KeyError:
    logger.critical("Unable to generate report, not enough logs")
except Exception as e:
    logger.critical("Unable to generate report: " + str(e))
